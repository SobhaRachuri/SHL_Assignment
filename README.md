# SHL_Assignment
SHL Assessment Recommendation System 

# ğŸ§  SHL Assessment Recommendation Engine

This project is a **Web-based Retrieval-Augmented Generation (RAG)** tool built using **Streamlit**, designed to recommend SHL assessments based on natural language job descriptions or queries.

---

## ğŸš€ Demo

ğŸŒ [Live Web App] https://shlassignment-a9eizpcefsh3w42ppdkqrj.streamlit.app/  
ğŸ“‚ [GitHub Repository] https://github.com/SobhaRachuri

---

## ğŸ§° Features

- ğŸ“ Accepts natural language queries (e.g., JD or skill requirements).
- âš™ï¸ Uses embedding-based retrieval with cosine similarity.
- ğŸ“Š Evaluation metrics like **Mean Recall@3** and **MAP@3** implemented.
- ğŸ§  Optionally integrates LLMs (e.g., OpenAI, Gemini) to enhance relevance.
- ğŸ–¼ï¸ Clean UI using **Streamlit**.

---

## ğŸ”§ How It Works

1. User enters a query (e.g., JD or skills).
2. Query is encoded using embedding models.
3. Relevant SHL assessments are retrieved based on semantic similarity.
4. Optional: LLM summarizes the match rationale.
5. Evaluation done using benchmark queries and metrics.

---

## ğŸ“‚ Folder Structure
ğŸ“ SHL_Assignment/ â”œâ”€â”€ app.py # Main Streamlit app â”œâ”€â”€ evaluate.py # Evaluation metrics script (Recall@3, MAP@3) â”œâ”€â”€ data/ â”‚ â””â”€â”€ assessments.csv # SHL catalog or embedding data â”œâ”€â”€ utils/ â”‚ â””â”€â”€ rag_utils.py # Embedding, similarity, and helper functions â”œâ”€â”€ README.md # Project overview

## ğŸ§ª Evaluation Metrics

The solution has been tested using a benchmark set, and the following results were achieved:

- ğŸ¯ **Mean Recall@3**: `1.0`
- ğŸ“Š **MAP@3**: `1.0`

These high scores indicate the system's strong ability to retrieve relevant assessments accurately for given job descriptions.



